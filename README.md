# PaperCup Video Annotation API

This is the Backend API for the PaperCup Video Annotation Service.

# Architecture and Dependencies

This application is a TypeScript application utilizing the following technologies:
 * Execution Scripts: Yarn
 * API Framework: Express
 * Database Management: Prisma (with an SQLite Database - meant to be easily transferable to a more permanent RDBMS)
 * Containerization: Docker
 * Testing: Jest

The application has the following key features:
 * JSON API with 3 distinct models:
   * Video data with CRUD operations
   * Annotation data with CRUD operations
   * API Key data (for authorization not CRUD operations)

# Installation

Install the app with:
```
yarn install
```

While committing a database with a service is non-standard, the SQLite Prisma database has been included in this repository
for the sake of this test.

A fresh database can be created with (although a development sqlite db has been left in place):
```
npx prisma migrate <database_name>
```

The database name will define what the file in prisma is called for example:
```
npx prisma migrate dev
```

Will generate a ```prisma/dev.db``` file.

# Configuration

You will have to copy the .env.example file to either .env or .env.docker (depending on how you run the service).

Then, since we do not have the means of sharing the encryption key(s) you'll have to generate your own.

This will need to be set to the APIKEY_ENCRYPTION_KEY variable in the .env (or .env.docker) file.
A key can be generated by:
```
crypto.randomBytes(32).toString("hex");
```

Then generate an API Key you can use with the service requests (i.e. the Authorization Header):
```
yarn run generate-apikey
```

# Scripts

Execution scripts are found in the scripts section of the package.json file in the root of this project.

Starting the service locally for development:
```
yarn run start:dev
```

OR - start the service with docker:
```
docker-compose up
```


Running the integration tests:
```
yarn run test
```

# Database Management

All changes to the schema.prisma file will require applying the changes to the database schema.

You can add the database migration by running:

```
npx prisma migrate <database_name> --name <migration name>
```

# API Documentation

All authorization is done via the generated api keys.  Add it as a header to your requests:

```
Authorization: APIKey-V1 <key value>
```

## Endpoints

### Videos
 * /videos
  * Method: GET - get all videos and related annotations (no pagination has been added at this time)
 * /videos/:id
  * Method: GET - get a single video and its annotations
 * /videos:
  * Method: POST - create a video resource - conflict constraint to url value
  * Params: url, description, length (HH:MM:SS)
 * /videos:
  * Method: PUT - update an existing video

### Annotations
 * /annotations
  * Method: GET - get all annotations (no pagination has been added at this time)
 * /annotations/:id
  * Method: GET - get a single annotation
 * /annotations:
  * Method: POST - create an annotation resource - requires a pre-existing video id
  * Params: videoId, startTime (HH:MM:SS), endTime (HH:MM:SS)
  * Optional Params: type, additionalNotes
 * /annotations:
  * Method: PUT - update an existing annotation


# Areas for expansion
 * Add automatic swagger API documentation generation / docs page / This could be done by converting the service to a TSOA controller format.
 * Add Unit Tests to everything
 * Replace the SQLite Database with another RDBMS
 * Expand the use of the API Key (include a deletion/expiry system onto them)
 * Add IaC (i.e. Terraform, etc) to this repo for deployment
 * Add CI/CD pipelines (e.g. github actions)
 * Expand error handling to address all possible use cases (e.g. gracefully handle invalid IDs)
 * Add commit hooks for linting and automating local unit tests
 * Resources should really only be made available to the creator and/or who they give resources to
 * A user system actually makes more sense now than the API Key

# Assumptions
 * Users will want to work in 'HH:MM:SS' rather than milliseconds, so a converstion was necessary.
 * We will eventually want to move this to a more extensible RDBMS so I tried to set this up so it can be converted more easily in the future.
 * The length of the video had to be explicitly passed in rather than extracting it from the video file (which is probably more likely)
 * The SQLite database has been left in place to make it easier for someone to have a poke around what the service does (this is NOT standard practice).
 * Audit fields (createdAt and updatedAt) are accepted standard additions to the schema

# Contact

Author: Gerik Peterson

Email: Gerik@GerikPeterson.com

License: MIT
